
from huggingface_hub import InferenceClient
from constants import LLM_PROMPT, LLM_PROMPT_FOLLOWUP_CHAT


def perform_llm_article_analysis(title, body, image_caption, metadata, llm_hf_id="microsoft/Phi-3-mini-4k-instruct"):
    """
    Performs article analysis using a specified large language model.
    :param title: The title of the article
    :param body: The main content or body of the article
    :param image_caption: A caption describing the primary image in the article
    :param metadata: Metadata associated with the primary image
    :param llm_hf_id: The Hugging Face ID of the LLM to use for inference
    :return: The response generated by the LLM based on the article analysis
    """

    prompt = LLM_PROMPT \
        .replace("$TITLE$", title) \
        .replace("$BODY$", body) \
        .replace("$IMAGE_CAPTION$", image_caption) \
        .replace("$METADATA$", metadata)

    return __perform_llm_inference(prompt, llm_hf_id=llm_hf_id)


def perform_llm_inference_chat_followup(current_reasoning, message, llm_hf_id="microsoft/Phi-3-mini-4k-instruct"):
    """
    Performs a follow-up chat inference with a large language model.
    :param current_reasoning: String that provides additional context or reasoning to the model during inference
    :param message: String, the user's latest input message
    :param llm_hf_id: The Hugging Face ID of the LLM to use for inference
    :return: The response generated by the LLM for the follow-up chat
    """

    prompt = LLM_PROMPT_FOLLOWUP_CHAT \
        .replace("$CURRENT_REASONING$", current_reasoning) \
        .replace("$MESSAGE$", message)

    return __perform_llm_inference(prompt, llm_hf_id=llm_hf_id)


def __perform_llm_inference(prompt, llm_hf_id="microsoft/Phi-3-mini-4k-instruct"):
    """
    Sends a prompt to a specified large language model (LLM) for inference.
    :param prompt:  The input prompt to send to the LLM for processing
    :param llm_hf_id: The Hugging Face ID of the LLM to use for inference
    :return: The complete response generated by the LLM based on the given prompt.
    """

    client = InferenceClient(
        llm_hf_id,
        token="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",
    )

    response = ""
    for message in client.chat_completion(
        messages=[{"role": "user", "content": prompt}],
        max_tokens=500,
        stream=True,
    ):
        response += message.choices[0].delta.content

    return response
